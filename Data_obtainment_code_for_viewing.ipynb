{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My original project was to compare the comment sections of both CinemaSins and CinemaWins.  However, I needed to make sure that I only compared the comment sections of videos in which both channels covered the same movie.This notebook is a summary of my code that I used to identify those videos and then subsequently obtained the comments in those videos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np      \n",
    "import regex as re\n",
    "from random import sample\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n",
    " - The following table was made to keep track of the API call keywords and what other information they were related to that I needed to know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Method         |Id/Filters Available        |Parts Available              |Other Parameters                     |\n",
    "|---------------|----------------------------|-----------------------------|-------------------------------------|\n",
    "|channels       |categoryId, id              |               statistics, id|maxRestuls                           |\n",
    "|comments       |parentId, id                |                           id|maxResults,textFormat                |\n",
    "|commentThreads |allThreadsRelatedToChannelId, videoId|                  id|n/a                                  |\n",
    "|guideCategories|regionCode, id              |                          n/a|n/a                                  |\n",
    "|search         |videoCategoryId,relatedToVideoId|                      n/a|type, order, topicId, videoCategoryId|\n",
    "|subscriptions  |channelId, id       |contentDetails, subscriberSnippet, id|forChannelId, maxResults, order      |\n",
    "|videoCategories|regionCode, id              |                          n/a|n/a                                  |\n",
    "|videos         |mostPopular, id             |  statistics, suggestions, id|n/a                                  |\n",
    "\n",
    "\n",
    " - Extra: CHANNEL PARTS: contentDetails, contentOwnerDetails,status, topicDetails\n",
    " - Extra: VIDEO PARTS: contentDetails, fileDetails,id, liveStreamingDetails, player,snippet,statistics, status, suggestions, topicDetails\n",
    " - Extra: SEARCH PARAMS: type, maxResults, channelId, order, publishedAfter/Before, q (aka query term), topicId, videoCaption, videoCategoryId, videoDuration\n",
    " - Search order: date, rating, relevance, title, videoCount, viewCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used primarily to discover the Youtube API and figure out what works and doesn't.\n",
    "params = {}\n",
    "def Youtube_API(method, params = params):\n",
    "    endpoint = 'https://www.googleapis.com/youtube/v3/' + str(method)\n",
    "    params['key'] = 'AIzaSyCruFbHTolS_lK_AHrakQrjSVLzdEO5MaI'\n",
    "    res = requests.get(url = endpoint,params = params)\n",
    "#    if int(str(res)[11:14]) == 200 & verbose == 1:\n",
    "#        display(res.json())\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used as a jumping off point for later\n",
    "def obtain_video_comments_listed(VideoId, max_comments = 10000):\n",
    "    list_of_items = []                                       # Instantiate list_of_items\n",
    "    endpoint = 'https://www.googleapis.com/youtube/v3/commentThreads'\n",
    "    first_params = {                                         # Instantiate first Parameters\n",
    "        'part':'id,snippet',\n",
    "        'videoId':str(VideoId),\n",
    "        'key':'AIzaSyCruFbHTolS_lK_AHrakQrjSVLzdEO5MaI'\n",
    "    }\n",
    "    params = first_params                                    # Set params equal to first parameters\n",
    "    \n",
    "    for page in range(int(max_comments/20)):                 # For the number of pulls\n",
    "        try:\n",
    "            res=requests.get(url=endpoint,params=params)     # Makes requests, starting with 1st\n",
    "            for i in range(20):\n",
    "                list_of_items.append(res.json()['items'][i]) # Adds each item to list_of_items\n",
    "            next_page = res.json()['nextPageToken']          # Sets the next_page Token\n",
    "            params = {                                       # Re-defines parameters to pull next page\n",
    "                'part':'id,snippet',\n",
    "                'videoId':str(VideoId),\n",
    "                'key':'AIzaSyCruFbHTolS_lK_AHrakQrjSVLzdEO5MaI',\n",
    "                'pageToken' : next_page\n",
    "            }\n",
    "            print(\"Page#:\", page+1, \"comments:\", (page+1)*20, \"Next:\", next_page[8:18]) # Sanity Check\n",
    "\n",
    "        except:                                               # Try/Except to ensure we stop at end\n",
    "            print('Limit likely hit.  Returning available posts.')\n",
    "            break\n",
    "        \n",
    "    #Turning list into dictionaries:\n",
    "    list_of_dicts = []\n",
    "    for item in range(len(list_of_items)):\n",
    "        quick_dict = {\n",
    "            \"comment_id\": list_of_items[item]['id'],\n",
    "            \"replies\": list_of_items[item]['snippet']['totalReplyCount'],\n",
    "            'author_id': list_of_items[item]['snippet']['topLevelComment']['snippet']['authorChannelId']['value'],\n",
    "            'author_name': list_of_items[item]['snippet']['topLevelComment']['snippet']['authorDisplayName'],\n",
    "            'likes': list_of_items[item]['snippet']['topLevelComment']['snippet']['likeCount'],\n",
    "            'published_time': list_of_items[item]['snippet']['topLevelComment']['snippet']['publishedAt'],\n",
    "            'text': list_of_items[item]['snippet']['topLevelComment']['snippet']['textOriginal'],\n",
    "            'video_id': list_of_items[item]['snippet']['topLevelComment']['snippet']['videoId']\n",
    "            }\n",
    "        list_of_dicts.append(quick_dict)\n",
    "    return list_of_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of function for pulling (hopefully) all the videos for a channel\n",
    "def obtain_videos(channelId):\n",
    "    video_list = []\n",
    "    params = {\n",
    "        'part':'snippet',\n",
    "        'channelId': str(channelId),\n",
    "        'maxResults': '50'}\n",
    "    for page in range(100):                                  # Pointlessly High Number\n",
    "        try:                                                 # Try/Except to ensure we stop at end\n",
    "            res = Youtube_API('search', params)              # Uses prev func to search API \n",
    "            for i in range(50):                              # for each item in the page\n",
    "                if res.json()['items'][i]['id']['kind'] == 'youtube#video': # Removing non-videos\n",
    "                    video_list.append(res.json()['items'][i])# Appending videos to video_list\n",
    "            next_page = res.json()['nextPageToken']          # Sets the next_page Token\n",
    "            params = {                                       # Re-defines parameters to pull next page\n",
    "                'part':'snippet',\n",
    "                'channelId': str(channelId),\n",
    "                'maxResults': '50',\n",
    "                'pageToken' : str(next_page)}\n",
    "            print(\"Page#:\", page+1)                           # Sanity Check\n",
    "        except:                                               # Try/Except to ensure we stop at end\n",
    "            print('Limit hit! Returning videos')\n",
    "            break\n",
    "    return video_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining Comments for same comments in both Cinema's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(w/s)ins_video_list - List of all videos and their info.  Raw.\n",
    "\n",
    "\n",
    "(w/s)ins_titles     - List of all names of wins videos in order of Video List\n",
    "\n",
    "(w/s)ins_tuples     - List of all names of wins videos and thie place in Video list in order of video list\n",
    "(w/s)ins_cypher     - Only the numbers of videos in video list that are shared with Sins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page#: 1\n",
      "Page#: 2\n",
      "Page#: 3\n",
      "Limit hit! Returning videos\n"
     ]
    }
   ],
   "source": [
    "wins_video_list = obtain_videos('UCL8h3ri2WN_-IbviBlWtUcQ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page#: 1\n",
      "Page#: 2\n",
      "Page#: 3\n",
      "Page#: 4\n",
      "Page#: 5\n",
      "Page#: 6\n",
      "Page#: 7\n",
      "Limit hit! Returning videos\n"
     ]
    }
   ],
   "source": [
    "sins_video_list = obtain_videos('UCYUQQgogVeQY8cMQamhHJcg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiding_code = sample(range(0,30),30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Wins Titles Tuples List\n",
    "wins_tuples = []\n",
    "count = 0                                                   # Count will be the video number\n",
    "hiding_code = sample(range(100,999),30)\n",
    "hide_num = 1\n",
    "for video in wins_video_list:  \n",
    "    if video['snippet']['title'][0] == 'E':                 # If it's viable video\n",
    "        title = video['snippet']['title']                   # Defining the title\n",
    "        title = title.replace(\"Everything GREAT About \",\"\") # Removing start of title\n",
    "        title = title.strip('!')                            # Removing Trailing Exclimation Point\n",
    "        wins_tuples.append((count,title))\n",
    "    else:\n",
    "        wins_tuples.append((count,\n",
    "                'Other Video' + str(hiding_code[hide_num])))\n",
    "        hide_num += 1\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Sins Titles Tuples List\n",
    "sins_tuples = []\n",
    "count = 0\n",
    "for video in sins_video_list:  \n",
    "    if video['snippet']['title'][0] == 'E':                 # If it's viable video\n",
    "        title = video['snippet']['title']                   # Defining the title\n",
    "        title = title.replace(\"Everything Wrong With \",\"\")  # Removing start of title\n",
    "        title = title.replace(\" Minutes Or Less\",\"\")        # Removing most of end of title\n",
    "        if title[-5:-3] == 'In':                            # Removing \" In ##\"\n",
    "            title = title[:-6]\n",
    "        elif title[-4:-2] == 'In':\n",
    "            title = title[:-5]\n",
    "        sins_tuples.append((count,title))\n",
    "    else:\n",
    "        sins_tuples.append((count,\n",
    "            'Other Video' + str(hiding_code[hide_num])))\n",
    "        hide_num += 1\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following was used to check by human eye to make sure that things were linning up and working out properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['After Earth', 'Ant-Man', 'Avengers: Age of Ultron', 'Baby Driver', 'Batman v Superman: Dawn of Justice', 'Big Hero 6', 'Black Panther', 'Captain America: Civil War', 'Captain America: The First Avenger', 'Captain America: The Winter Soldier', 'Coco', 'Deadpool', 'Deadpool 2', \"Ender's Game\", 'Finding Dory', 'Finding Nemo', 'Home Alone', 'Inside Out', 'Jumanji: Welcome to the Jungle', 'Jurassic World', 'Kingsman: The Golden Circle', 'Kung Fu Panda', 'Logan', 'Megamind', 'Moana', 'Pacific Rim', 'Ready Player One', 'Rogue One: A Star Wars Story', 'Spider-Man', 'Spider-Man 2', 'Spider-Man 3', 'Spider-Man: Homecoming', 'Star Trek Beyond', 'Star Trek Into Darkness', 'Star Wars: Episode VII - The Force Awakens', 'Suicide Squad', 'The Avengers', 'The Bourne Identity', 'The Dark Knight', 'The Dark Knight Rises', 'The Equalizer', 'The Hunger Games', 'The Incredibles', 'The Jungle Book', 'The Last Airbender', 'The Lego Movie', 'The Maze Runner', 'The Wolverine', 'Trolls', 'Warcraft', 'Warm Bodies', 'Watchmen', 'Wonder Woman', 'Wreck-It Ralph', 'Zootopia']\n",
      "['After Earth', 'Ant-Man', 'Avengers: Age of Ultron', 'Baby Driver', 'Batman v Superman: Dawn of Justice', 'Big Hero 6', 'Black Panther', 'Captain America: Civil War', 'Captain America: The First Avenger', 'Captain America: The Winter Soldier', 'Coco', 'Deadpool', 'Deadpool 2', \"Ender's Game\", 'Finding Dory', 'Finding Nemo', 'Home Alone', 'Inside Out', 'Jumanji: Welcome to the Jungle', 'Jurassic World', 'Kingsman: The Golden Circle', 'Kung Fu Panda', 'Logan', 'Megamind', 'Moana', 'Pacific Rim', 'Ready Player One', 'Rogue One: A Star Wars Story', 'Spider-Man', 'Spider-Man 2', 'Spider-Man 3', 'Spider-Man: Homecoming', 'Star Trek Beyond', 'Star Trek Into Darkness', 'Star Wars: Episode VII - The Force Awakens', 'Suicide Squad', 'The Avengers', 'The Bourne Identity', 'The Dark Knight', 'The Dark Knight Rises', 'The Equalizer', 'The Hunger Games', 'The Incredibles', 'The Jungle Book', 'The Last Airbender', 'The Lego Movie', 'The Maze Runner', 'The Wolverine', 'Trolls', 'Warcraft', 'Warm Bodies', 'Watchmen', 'Wonder Woman', 'Wreck-It Ralph', 'Zootopia']\n"
     ]
    }
   ],
   "source": [
    "print(sorted([win[1] for win in wins_tuples if win[1] in [sin[1] for sin in sins_tuples]]))\n",
    "print(sorted([sin[1] for sin in sins_tuples if sin[1] in [win[1] for win in wins_tuples]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 6, 7, 9, 11, 18, 20, 23, 24, 25, 26, 27, 28, 31, 32, 33, 39, 40, 41, 42, 43, 48, 57, 58, 60, 61, 65, 67, 72, 74, 77, 80, 81, 82, 86, 89, 90, 92, 93, 97, 98, 100, 104, 118, 120, 121, 124, 125, 130, 134, 140, 141, 142]\n",
      "[3, 9, 11, 13, 23, 27, 29, 30, 36, 47, 58, 85, 88, 90, 99, 104, 107, 117, 121, 128, 130, 131, 132, 135, 149, 154, 161, 164, 170, 190, 198, 200, 220, 225, 228, 235, 237, 251, 253, 277, 287, 290, 291, 299, 300, 308, 309, 311, 314, 315, 323, 331, 345, 353, 359]\n"
     ]
    }
   ],
   "source": [
    "wins_cypher = [win[0] for win in wins_tuples if win[1] in [sin[1] for sin in sins_tuples]]\n",
    "sins_cypher = [sin[0] for sin in sins_tuples if sin[1] in [win[1] for win in wins_tuples]]\n",
    "print(sorted(wins_cypher)) \n",
    "print(sorted(sins_cypher))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Wins Video Titles List\n",
    "wins_titles = []\n",
    "for video in wins_video_list:  \n",
    "    if video['id']['kind'] == 'youtube#video':                          # If it's a video...\n",
    "        if video['snippet']['title'][0] == 'E':                         # If it's viable\n",
    "            title = video['snippet']['title']                           # Defining the title\n",
    "            title = title.replace(\"Everything GREAT About \",\"\")         # Removing start of title\n",
    "            title = title.strip('!')\n",
    "            wins_titles.append(title)\n",
    "        else:\n",
    "            wins_titles.append('Other Video')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Wins Video Titles List\n",
    "sins_titles = []\n",
    "for video in sins_video_list:  \n",
    "    if video['id']['kind'] == 'youtube#video':                          # If it's a video...\n",
    "        if video['snippet']['title'][0] == 'E':                         # If it's viable\n",
    "            title = video['snippet']['title']                           # Defining the title\n",
    "            title = title.replace(\"Everything Wrong With \",\"\")          # Removing start of title\n",
    "            title = title.replace(\" Minutes Or Less\",\"\")                # Removing most of end of title\n",
    "            if title[-5:-3] == 'In':                                    # Removing \" In ##\"\n",
    "                title = title[:-6]\n",
    "            elif title[-4:-2] == 'In':\n",
    "                title = title[:-5]\n",
    "            sins_titles.append(title)\n",
    "        else:\n",
    "            sins_titles.append('Other Video')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two cells were used to make sure that all of the bojects in each list were alligned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sanity Check that the wins cypher list works\n",
    "count = 0\n",
    "for cypher in sorted(wins_cypher):\n",
    "    print('cypher: ',wins_cypher[count])\n",
    "    count += 1\n",
    "    print('count: ',count)\n",
    "    print('                      ', wins_titles[cypher])\n",
    "    print('                ', wins_tuples[cypher])\n",
    "    print(wins_video_list[cypher]['snippet']['title'])\n",
    "    print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check that the sins cypher list works\n",
    "count = 0\n",
    "for cypher in sorted(sins_cypher):\n",
    "    print('cypher: ',sins_cypher[count])\n",
    "    count += 1\n",
    "    print('count: ',count)\n",
    "    print('                     ', sins_titles[cypher])\n",
    "    print('              ',sins_tuples[cypher])\n",
    "    print(sins_video_list[cypher]['snippet']['title'])\n",
    "#    print(sins_video_list[cypher]['id']['videoId'])\n",
    "    print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two cells actually collected the comments from all vieos that we were looking for and printed out what video it was on at each step.  The outputs have been cleared for ease of reading but for example, the first few lines of the following cell were:\n",
    "\n",
    "The Avengers !!!\n",
    "\n",
    "CWVaNzGpQI8\n",
    "\n",
    "Page#: 1 comments: 20 Next: MVVvYTdDNE\n",
    "\n",
    "Page#: 2 comments: 40 Next: MklIZ3gwWH\n",
    "\n",
    "Page#: 3 comments: 60 Next: MUhuSUM2UX\n",
    "\n",
    "Page#: 4 comments: 80 Next: ME9EVEJfdF\n",
    "\n",
    "Page#: 5 comments: 100 Next: MFU5c1EtME\n",
    "\n",
    "Page#: 6 comments: 120 Next: MzRBRjdtbU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# WINS with Sins!\n",
    "list_of_items = []                                           # Instantiate list_of_items\n",
    "endpoint = 'https://www.googleapis.com/youtube/v3/commentThreads' \n",
    "for cypher in sorted(wins_cypher):\n",
    "    VideoId = wins_video_list[cypher]['id']['videoId']\n",
    "    first_params = {                                         # Instantiate first Parameters\n",
    "    'part':'id,snippet',\n",
    "    'videoId':str(VideoId),\n",
    "    'key':'AIzaSyCruFbHTolS_lK_AHrakQrjSVLzdEO5MaI'\n",
    "    }\n",
    "    params = first_params  \n",
    "    print(wins_titles[cypher],'!!!')\n",
    "    print(wins_video_list[cypher]['id']['videoId'])\n",
    "    for page in range(int(50000/20)):                        # For the number of pulls\n",
    "        try:\n",
    "            res=requests.get(url=endpoint,params=params)     # Makes requests, starting with 1st\n",
    "            for i in range(20):\n",
    "                list_of_items.append(res.json()['items'][i]) # Adds each item to list_of_items\n",
    "            next_page = res.json()['nextPageToken']          # Sets the next_page Token\n",
    "            params = {                                       # Re-defines parameters to pull next page\n",
    "                'part':'id,snippet',\n",
    "                'videoId':str(VideoId),\n",
    "                'key':'AIzaSyCruFbHTolS_lK_AHrakQrjSVLzdEO5MaI',\n",
    "                'pageToken' : next_page\n",
    "            }\n",
    "            print(\"Page#:\", page+1, \"comments:\", (page+1)*20) # Sanity Check\n",
    "\n",
    "        except:                                               # Try/Except to ensure we stop at end\n",
    "            print('Limit likely hit. Returning available posts.')\n",
    "            break\n",
    "list_of_dicts = []\n",
    "for item in range(len(list_of_items)):\n",
    "    quick_dict = {\n",
    "        \"comment_id\": list_of_items[item]['id'],\n",
    "        \"replies\": list_of_items[item]['snippet']['totalReplyCount'],\n",
    "        'author_id': list_of_items[item]['snippet']['topLevelComment']['snippet']['authorChannelId']['value'],\n",
    "        'author_name': list_of_items[item]['snippet']['topLevelComment']['snippet']['authorDisplayName'],\n",
    "        'likes': list_of_items[item]['snippet']['topLevelComment']['snippet']['likeCount'],\n",
    "        'published_time': list_of_items[item]['snippet']['topLevelComment']['snippet']['publishedAt'],\n",
    "        'text': list_of_items[item]['snippet']['topLevelComment']['snippet']['textOriginal'],\n",
    "        'video_id': list_of_items[item]['snippet']['topLevelComment']['snippet']['videoId']\n",
    "        }\n",
    "    list_of_dicts.append(quick_dict)\n",
    "wins_df = pd.DataFrame(list_of_dicts, columns = ['text', 'likes', 'replies', 'published_time', \n",
    "                                                 'comment_id', 'author_id', 'author_name', 'video_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SINS with Wins!\n",
    "list_of_items = []                                       # Instantiate list_of_items\n",
    "endpoint = 'https://www.googleapis.com/youtube/v3/commentThreads'\n",
    "params = first_params  \n",
    "for cypher in sorted(sins_cypher):\n",
    "    VideoId = sins_video_list[cypher]['id']['videoId']\n",
    "    first_params = {                                         # Instantiate first Parameters\n",
    "    'part':'id,snippet',\n",
    "    'videoId':str(VideoId),\n",
    "    'key':'AIzaSyCruFbHTolS_lK_AHrakQrjSVLzdEO5MaI'\n",
    "    }\n",
    "    params = first_params  \n",
    "    print(sins_titles[cypher],'!!!')\n",
    "    print(sins_video_list[cypher]['id']['videoId'])\n",
    "    for page in range(int(50000/20)):                 # For the number of pulls\n",
    "        try:\n",
    "            res=requests.get(url=endpoint,params=params)     # Makes requests, starting with 1st\n",
    "            for i in range(20):\n",
    "                list_of_items.append(res.json()['items'][i]) # Adds each item to list_of_items\n",
    "            next_page = res.json()['nextPageToken']          # Sets the next_page Token\n",
    "            params = {                                       # Re-defines parameters to pull next page\n",
    "                'part':'id,snippet',\n",
    "                'videoId':str(VideoId),\n",
    "                'key':'AIzaSyCruFbHTolS_lK_AHrakQrjSVLzdEO5MaI',\n",
    "                'pageToken' : next_page\n",
    "            }\n",
    "            print(\"Page#:\", page+1, \"comments:\", (page+1)*20) # Sanity Check\n",
    "\n",
    "        except:                                               # Try/Except to ensure we stop at end\n",
    "            print('Limit likely hit.  Returning available posts.')\n",
    "            break\n",
    "list_of_dicts = []\n",
    "for item in range(len(list_of_items)):\n",
    "    quick_dict = {\n",
    "        \"comment_id\": list_of_items[item]['id'],\n",
    "        \"replies\": list_of_items[item]['snippet']['totalReplyCount'],\n",
    "        'author_id': list_of_items[item]['snippet']['topLevelComment']['snippet']['authorChannelId']['value'],\n",
    "        'author_name': list_of_items[item]['snippet']['topLevelComment']['snippet']['authorDisplayName'],\n",
    "        'likes': list_of_items[item]['snippet']['topLevelComment']['snippet']['likeCount'],\n",
    "        'published_time': list_of_items[item]['snippet']['topLevelComment']['snippet']['publishedAt'],\n",
    "        'text': list_of_items[item]['snippet']['topLevelComment']['snippet']['textOriginal'],\n",
    "        'video_id': list_of_items[item]['snippet']['topLevelComment']['snippet']['videoId']\n",
    "        }\n",
    "    list_of_dicts.append(quick_dict)\n",
    "sins_df = pd.DataFrame(list_of_dicts, columns = ['text', 'likes', 'replies', 'published_time', \n",
    "                                                 'comment_id', 'author_id', 'author_name', 'video_id'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
