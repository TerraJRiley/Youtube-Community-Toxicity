{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stuff to do\n",
    "Original Problem Statement: Through sentimitation, can we learn which channels have the most positive and negative communities?  By identifying which channels are putting out more positivity and which are putting out less positivity will help creaters improve on such things should they wish to.  It will also help Youtube support the channels which or more positive, which will help improve the brand of youtube by making it present more as a place which can foster positivity rather than a cesspool of potential negativity.\n",
    "#### Bonuses\n",
    " - Latient Dirilech Analysis\n",
    " - Topic system\n",
    " - Function to guess/estimate how many comments a channel has\n",
    " - Model prediction how many comments will happen over time (based on views and topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np      \n",
    "import regex as re\n",
    "from random import sample\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Method         |Id/Filters Available        |Parts Available              |Other Parameters                     |\n",
    "|---------------|----------------------------|-----------------------------|-------------------------------------|\n",
    "|channels       |categoryId, id              |               statistics, id|maxRestuls                           |\n",
    "|comments       |parentId, id                |                           id|maxResults,textFormat                |\n",
    "|commentThreads |allThreadsRelatedToChannelId, videoId|                  id|n/a                                  |\n",
    "|guideCategories|regionCode, id              |                          n/a|n/a                                  |\n",
    "|search         |videoCategoryId,relatedToVideoId|                      n/a|type, order, topicId, videoCategoryId|\n",
    "|subscriptions  |channelId, id       |contentDetails, subscriberSnippet, id|forChannelId, maxResults, order      |\n",
    "|videoCategories|regionCode, id              |                          n/a|n/a                                  |\n",
    "|videos         |mostPopular, id             |  statistics, suggestions, id|n/a                                  |\n",
    "\n",
    "\n",
    " - Extra: CHANNEL PARTS: contentDetails, contentOwnerDetails,status, topicDetails\n",
    " - Extra: VIDEO PARTS: contentDetails, fileDetails,id, liveStreamingDetails, player,snippet,statistics, status, suggestions, topicDetails\n",
    " - Extra: SEARCH PARAMS: type, maxResults, channelId, order, publishedAfter/Before, q (aka query term), topicId, videoCaption, videoCategoryId, videoDuration\n",
    " - Search order: date, rating, relevance, title, videoCount, viewCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used primarily to discover the Youtube API and figure out what works and doesn't.\n",
    "params = {}\n",
    "def Youtube_API(method, params = params):\n",
    "    endpoint = 'https://www.googleapis.com/youtube/v3/' + str(method)\n",
    "    params['key'] = 'AIzaSyCruFbHTolS_lK_AHrakQrjSVLzdEO5MaI'\n",
    "    res = requests.get(url = endpoint,params = params)\n",
    "#    if int(str(res)[11:14]) == 200 & verbose == 1:\n",
    "#        display(res.json())\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_channel_comments(ChannelID, max_comments = 1000000):\n",
    "    list_of_items = []                                       # Instantiate list_of_items\n",
    "    endpoint = 'https://www.googleapis.com/youtube/v3/commentThreads'\n",
    "    \n",
    "    first_params = {                                         # Instantiate first Parameters\n",
    "        'part':'id,snippet',\n",
    "        'allThreadsRelatedToChannelId':str(ChannelID),\n",
    "        'key':'AIzaSyCruFbHTolS_lK_AHrakQrjSVLzdEO5MaI'\n",
    "    }\n",
    "    params = first_params                                    # Set params equal to first parameters\n",
    "    \n",
    "    res=requests.get(url=endpoint,params=first_params)       # Here incase we want a get check in future\n",
    "    if int(str(res)[11:14]) == 200:\n",
    "        print('All good.  Grabbing first page...')\n",
    "    else:\n",
    "        print(\"Responce wasn't 200, please address.\")\n",
    "        return \n",
    "    \n",
    "    for page in range(int(max_comments/20)):                  # For the number of pulls\n",
    "        try:\n",
    "            res=requests.get(url=endpoint,params=params)      # Makes requests, starting with the first\n",
    "            for i in range(res.json()['pageInfo']['totalResults']):\n",
    "                list_of_items.append(res.json()['items'][i])  # Adds each item to list_of_items, individually\n",
    "            next_page = res.json()['nextPageToken']           # Sets the next_page Token\n",
    "            params = {                                        # Re-defines the parameters to pull next page\n",
    "                'part':'id,snippet',\n",
    "                'allThreadsRelatedToChannelId':str(ChannelID),\n",
    "                'key':'AIzaSyCruFbHTolS_lK_AHrakQrjSVLzdEO5MaI',\n",
    "                'pageToken' : next_page\n",
    "            }                                                 #\"Next:\", next_page[8:18])   # Sanity Check\n",
    "            print(\"Page#:\", page+1, \"Comments:\", (page+1)*20,\n",
    "                  res.json()['items'][i]['snippet']['topLevelComment']['snippet']['publishedAt'])\n",
    "            if page % 10 == 0:\n",
    "                print('Next Page Token:', res.json()['nextPageToken'])\n",
    "\n",
    "        except:                                               # Try/Except to ensure we stop at end w/o issue\n",
    "            print('Limit likely hit.  Returning available posts.')\n",
    "            break\n",
    "        \n",
    "    #Turning list into dictionaries:\n",
    "    list_of_dicts = []\n",
    "    for item in range(len(list_of_items)):\n",
    "        quick_dict = {\n",
    "            \"comment_id\": list_of_items[item]['id'],\n",
    "            \"is_public\": list_of_items[item]['snippet']['isPublic'],\n",
    "            \"replies\": list_of_items[item]['snippet']['totalReplyCount'],\n",
    "            'author_name': list_of_items[item]['snippet']['topLevelComment']['snippet']['authorDisplayName'],\n",
    "            'likes': list_of_items[item]['snippet']['topLevelComment']['snippet']['likeCount'],\n",
    "            'published_time': list_of_items[item]['snippet']['topLevelComment']['snippet']['publishedAt'],\n",
    "            'text': list_of_items[item]['snippet']['topLevelComment']['snippet']['textOriginal']\n",
    "            }\n",
    "        try:\n",
    "            quick_dict['author_id'] = list_of_items[item]['snippet']['topLevelComment']['snippet']['authorChannelId']['value']\n",
    "        except:\n",
    "            quick_dict['author_id'] = 'None'\n",
    "        try:\n",
    "            quick_dict['video_id']  = list_of_items[item]['snippet']['topLevelComment']['snippet']['videoId']\n",
    "        except:\n",
    "            quick_dict['video_id']  = 'None'\n",
    "            \n",
    "        list_of_dicts.append(quick_dict)\n",
    "    return pd.DataFrame(list_of_dicts, columns = ['text', 'likes', 'replies', 'published_time',\n",
    "                                                  'author_name', 'author_id', 'video_id', 'is_public', 'comment_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_video_comments_listed(VideoId, max_comments = 10000):\n",
    "    list_of_items = []                                       # Instantiate list_of_items\n",
    "    endpoint = 'https://www.googleapis.com/youtube/v3/commentThreads'\n",
    "    first_params = {                                         # Instantiate first Parameters\n",
    "        'part':'id,snippet',\n",
    "        'videoId':str(VideoId),\n",
    "        'key':'AIzaSyCruFbHTolS_lK_AHrakQrjSVLzdEO5MaI'\n",
    "    }\n",
    "    params = first_params                                    # Set params equal to first parameters\n",
    "    \n",
    "    for page in range(int(max_comments/20)):                 # For the number of pulls\n",
    "        try:\n",
    "            res=requests.get(url=endpoint,params=params)     # Makes requests, starting with 1st\n",
    "            for i in range(20):\n",
    "                list_of_items.append(res.json()['items'][i]) # Adds each item to list_of_items\n",
    "            next_page = res.json()['nextPageToken']          # Sets the next_page Token\n",
    "            params = {                                       # Re-defines parameters to pull next page\n",
    "                'part':'id,snippet',\n",
    "                'videoId':str(VideoId),\n",
    "                'key':'AIzaSyCruFbHTolS_lK_AHrakQrjSVLzdEO5MaI',\n",
    "                'pageToken' : next_page\n",
    "            }\n",
    "            print(\"Page#:\", page+1, \"comments:\", (page+1)*20, \"Next:\", next_page[8:18]) # Sanity Check\n",
    "\n",
    "        except:                                               # Try/Except to ensure we stop at end\n",
    "            print('Limit likely hit.  Returning available posts.')\n",
    "            break\n",
    "        \n",
    "    #Turning list into dictionaries:\n",
    "    list_of_dicts = []\n",
    "    for item in range(len(list_of_items)):\n",
    "        quick_dict = {\n",
    "            \"comment_id\": list_of_items[item]['id'],\n",
    "            \"replies\": list_of_items[item]['snippet']['totalReplyCount'],\n",
    "            'author_id': list_of_items[item]['snippet']['topLevelComment']['snippet']['authorChannelId']['value'],\n",
    "            'author_name': list_of_items[item]['snippet']['topLevelComment']['snippet']['authorDisplayName'],\n",
    "            'likes': list_of_items[item]['snippet']['topLevelComment']['snippet']['likeCount'],\n",
    "            'published_time': list_of_items[item]['snippet']['topLevelComment']['snippet']['publishedAt'],\n",
    "            'text': list_of_items[item]['snippet']['topLevelComment']['snippet']['textOriginal'],\n",
    "            'video_id': list_of_items[item]['snippet']['topLevelComment']['snippet']['videoId']\n",
    "            }\n",
    "        list_of_dicts.append(quick_dict)\n",
    "    return list_of_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_video_comments(VideoId, max_comments = 10000):\n",
    "    list_of_items = []                                       # Instantiate list_of_items\n",
    "    endpoint = 'https://www.googleapis.com/youtube/v3/commentThreads'\n",
    "    first_params = {                                         # Instantiate first Parameters\n",
    "        'part':'id,snippet',\n",
    "        'videoId':str(VideoId),\n",
    "        'key':'AIzaSyCruFbHTolS_lK_AHrakQrjSVLzdEO5MaI'\n",
    "    }\n",
    "    params = first_params                                    # Set params equal to first parameters\n",
    "    \n",
    "    for page in range(int(max_comments/20)):                 # For the number of pulls\n",
    "        try:\n",
    "            res=requests.get(url=endpoint,params=params)     # Makes requests, starting with 1st\n",
    "            for i in range(20):\n",
    "                list_of_items.append(res.json()['items'][i]) # Adds each item to list_of_items\n",
    "            next_page = res.json()['nextPageToken']          # Sets the next_page Token\n",
    "            params = {                                       # Re-defines parameters to pull next page\n",
    "                'part':'id,snippet',\n",
    "                'videoId':str(VideoId),\n",
    "                'key':'AIzaSyCruFbHTolS_lK_AHrakQrjSVLzdEO5MaI',\n",
    "                'pageToken' : next_page\n",
    "            }\n",
    "            print(\"Page#:\", page+1, \"comments:\", (page+1)*20, \"Next:\", next_page[8:18]) # Sanity Check\n",
    "#            time.sleep(3)                                    # Safety Rest\n",
    "\n",
    "        except:                                               # Try/Except to ensure we stop at end\n",
    "            print('Limit likely hit.  Returning available posts.')\n",
    "            break\n",
    "        \n",
    "    #Turning list into dictionaries:\n",
    "    list_of_dicts = []\n",
    "    for item in range(len(list_of_items)):\n",
    "        quick_dict = {\n",
    "            \"replies\": list_of_items[item]['snippet']['totalReplyCount'],\n",
    "            'likes': list_of_items[item]['snippet']['topLevelComment']['snippet']['likeCount'],\n",
    "            'published_time': list_of_items[item]['snippet']['topLevelComment']['snippet']['publishedAt'],\n",
    "            'text': list_of_items[item]['snippet']['topLevelComment']['snippet']['textOriginal'],\n",
    "           }\n",
    "        list_of_dicts.append(quick_dict)\n",
    "    return pd.DataFrame(list_of_dicts, columns = ['text', 'likes', 'replies', 'published_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of function for pulling (hopefully) all the videos for a channel\n",
    "def obtain_videos(channelId):\n",
    "    video_list = []\n",
    "    params = {\n",
    "        'part':'snippet',\n",
    "        'channelId': str(channelId),\n",
    "        'maxResults': '50'}\n",
    "    for page in range(100):                                  # Pointlessly High Number\n",
    "        try:                                                 # Try/Except to ensure we stop at end\n",
    "            res = Youtube_API('search', params)              # Uses prev func to search API \n",
    "            for i in range(50):                              # for each item in the page\n",
    "                if res.json()['items'][i]['id']['kind'] == 'youtube#video': # Removing non-videos\n",
    "                    video_list.append(res.json()['items'][i])# Appending videos to video_list\n",
    "            next_page = res.json()['nextPageToken']          # Sets the next_page Token\n",
    "            params = {                                       # Re-defines parameters to pull next page\n",
    "                'part':'snippet',\n",
    "                'channelId': str(channelId),\n",
    "                'maxResults': '50',\n",
    "                'pageToken' : str(next_page)}\n",
    "            print(\"Page#:\", page+1)                           # Sanity Check\n",
    "        except:                                               # Try/Except to ensure we stop at end\n",
    "            print('Limit hit! Returning videos')\n",
    "            break\n",
    "    return video_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for obtaining the title of a video based on the id\n",
    "def obtain_video_title(vid_id):\n",
    "    if vid_id == 'None':\n",
    "        return \"Discussion Comment\"\n",
    "    else:\n",
    "        endpoint = 'https://www.googleapis.com/youtube/v3/videos'\n",
    "        params = {\n",
    "            'part':'snippet',\n",
    "            'id':str(vid_id),\n",
    "            'key':'AIzaSyCruFbHTolS_lK_AHrakQrjSVLzdEO5MaI'\n",
    "        }\n",
    "\n",
    "        res = requests.get(url=endpoint,params=params)\n",
    "        return res.json()['items'][0]['snippet']['title']\n",
    "\n",
    "def Create_video_titles_column(df):\n",
    "    ids = sorted(set(df['video_id']))\n",
    "    dictionary = {}\n",
    "    for i in ids:\n",
    "\n",
    "        dictionary[i] = obtain_video_title(i)\n",
    "    return dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining Comments for same comments in both Cinema's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(w/s)ins_video_list - List of all videos and their info.  Raw.\n",
    "\n",
    "\n",
    "(w/s)ins_titles     - List of all names of wins videos in order of Video List\n",
    "\n",
    "(w/s)ins_tuples     - List of all names of wins videos and thie place in Video list in order of video list\n",
    "(w/s)ins_cypher     - Only the numbers of videos in video list that are shared with Sins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page#: 1\n",
      "Page#: 2\n",
      "Page#: 3\n",
      "Limit hit! Returning videos\n"
     ]
    }
   ],
   "source": [
    "wins_video_list = obtain_videos('UCL8h3ri2WN_-IbviBlWtUcQ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page#: 1\n",
      "Page#: 2\n",
      "Page#: 3\n",
      "Page#: 4\n",
      "Page#: 5\n",
      "Page#: 6\n",
      "Page#: 7\n",
      "Limit hit! Returning videos\n"
     ]
    }
   ],
   "source": [
    "sins_video_list = obtain_videos('UCYUQQgogVeQY8cMQamhHJcg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiding_code = sample(range(0,30),30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Wins Titles Tuples List\n",
    "wins_tuples = []\n",
    "count = 0                                                   # Count will be the video number\n",
    "hiding_code = sample(range(100,999),30)\n",
    "hide_num = 1\n",
    "for video in wins_video_list:  \n",
    "    if video['snippet']['title'][0] == 'E':                 # If it's viable video\n",
    "        title = video['snippet']['title']                   # Defining the title\n",
    "        title = title.replace(\"Everything GREAT About \",\"\") # Removing start of title\n",
    "        title = title.strip('!')                            # Removing Trailing Exclimation Point\n",
    "        wins_tuples.append((count,title))\n",
    "    else:\n",
    "        wins_tuples.append((count,\n",
    "                'Other Video' + str(hiding_code[hide_num])))\n",
    "        hide_num += 1\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Sins Titles Tuples List\n",
    "sins_tuples = []\n",
    "count = 0\n",
    "for video in sins_video_list:  \n",
    "    if video['snippet']['title'][0] == 'E':                 # If it's viable video\n",
    "        title = video['snippet']['title']                   # Defining the title\n",
    "        title = title.replace(\"Everything Wrong With \",\"\")  # Removing start of title\n",
    "        title = title.replace(\" Minutes Or Less\",\"\")        # Removing most of end of title\n",
    "        if title[-5:-3] == 'In':                            # Removing \" In ##\"\n",
    "            title = title[:-6]\n",
    "        elif title[-4:-2] == 'In':\n",
    "            title = title[:-5]\n",
    "        sins_tuples.append((count,title))\n",
    "    else:\n",
    "        sins_tuples.append((count,\n",
    "            'Other Video' + str(hiding_code[hide_num])))\n",
    "        hide_num += 1\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following was used to check by human eye to make sure that things were linning up and working out properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['After Earth', 'Ant-Man', 'Avengers: Age of Ultron', 'Baby Driver', 'Batman v Superman: Dawn of Justice', 'Big Hero 6', 'Black Panther', 'Captain America: Civil War', 'Captain America: The First Avenger', 'Captain America: The Winter Soldier', 'Coco', 'Deadpool', 'Deadpool 2', \"Ender's Game\", 'Finding Dory', 'Finding Nemo', 'Home Alone', 'Inside Out', 'Jumanji: Welcome to the Jungle', 'Jurassic World', 'Kingsman: The Golden Circle', 'Kung Fu Panda', 'Logan', 'Megamind', 'Moana', 'Pacific Rim', 'Ready Player One', 'Rogue One: A Star Wars Story', 'Spider-Man', 'Spider-Man 2', 'Spider-Man 3', 'Spider-Man: Homecoming', 'Star Trek Beyond', 'Star Trek Into Darkness', 'Star Wars: Episode VII - The Force Awakens', 'Suicide Squad', 'The Avengers', 'The Bourne Identity', 'The Dark Knight', 'The Dark Knight Rises', 'The Equalizer', 'The Hunger Games', 'The Incredibles', 'The Jungle Book', 'The Last Airbender', 'The Lego Movie', 'The Maze Runner', 'The Wolverine', 'Trolls', 'Warcraft', 'Warm Bodies', 'Watchmen', 'Wonder Woman', 'Wreck-It Ralph', 'Zootopia']\n",
      "['After Earth', 'Ant-Man', 'Avengers: Age of Ultron', 'Baby Driver', 'Batman v Superman: Dawn of Justice', 'Big Hero 6', 'Black Panther', 'Captain America: Civil War', 'Captain America: The First Avenger', 'Captain America: The Winter Soldier', 'Coco', 'Deadpool', 'Deadpool 2', \"Ender's Game\", 'Finding Dory', 'Finding Nemo', 'Home Alone', 'Inside Out', 'Jumanji: Welcome to the Jungle', 'Jurassic World', 'Kingsman: The Golden Circle', 'Kung Fu Panda', 'Logan', 'Megamind', 'Moana', 'Pacific Rim', 'Ready Player One', 'Rogue One: A Star Wars Story', 'Spider-Man', 'Spider-Man 2', 'Spider-Man 3', 'Spider-Man: Homecoming', 'Star Trek Beyond', 'Star Trek Into Darkness', 'Star Wars: Episode VII - The Force Awakens', 'Suicide Squad', 'The Avengers', 'The Bourne Identity', 'The Dark Knight', 'The Dark Knight Rises', 'The Equalizer', 'The Hunger Games', 'The Incredibles', 'The Jungle Book', 'The Last Airbender', 'The Lego Movie', 'The Maze Runner', 'The Wolverine', 'Trolls', 'Warcraft', 'Warm Bodies', 'Watchmen', 'Wonder Woman', 'Wreck-It Ralph', 'Zootopia']\n"
     ]
    }
   ],
   "source": [
    "print(sorted([win[1] for win in wins_tuples if win[1] in [sin[1] for sin in sins_tuples]]))\n",
    "print(sorted([sin[1] for sin in sins_tuples if sin[1] in [win[1] for win in wins_tuples]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 6, 7, 9, 11, 18, 20, 23, 24, 25, 26, 27, 28, 31, 32, 33, 39, 40, 41, 42, 43, 48, 57, 58, 60, 61, 65, 67, 72, 74, 77, 80, 81, 82, 86, 89, 90, 92, 93, 97, 98, 100, 104, 118, 120, 121, 124, 125, 130, 134, 140, 141, 142]\n",
      "[3, 9, 11, 13, 23, 27, 29, 30, 36, 47, 58, 85, 88, 90, 99, 104, 107, 117, 121, 128, 130, 131, 132, 135, 149, 154, 161, 164, 170, 190, 198, 200, 220, 225, 228, 235, 237, 251, 253, 277, 287, 290, 291, 299, 300, 308, 309, 311, 314, 315, 323, 331, 345, 353, 359]\n"
     ]
    }
   ],
   "source": [
    "wins_cypher = [win[0] for win in wins_tuples if win[1] in [sin[1] for sin in sins_tuples]]\n",
    "sins_cypher = [sin[0] for sin in sins_tuples if sin[1] in [win[1] for win in wins_tuples]]\n",
    "print(sorted(wins_cypher)) \n",
    "print(sorted(sins_cypher))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Wins Video Titles List\n",
    "wins_titles = []\n",
    "for video in wins_video_list:  \n",
    "    if video['id']['kind'] == 'youtube#video':                          # If it's a video...\n",
    "        if video['snippet']['title'][0] == 'E':                         # If it's viable\n",
    "            title = video['snippet']['title']                           # Defining the title\n",
    "            title = title.replace(\"Everything GREAT About \",\"\")         # Removing start of title\n",
    "            title = title.strip('!')\n",
    "            wins_titles.append(title)\n",
    "        else:\n",
    "            wins_titles.append('Other Video')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Wins Video Titles List\n",
    "sins_titles = []\n",
    "for video in sins_video_list:  \n",
    "    if video['id']['kind'] == 'youtube#video':                          # If it's a video...\n",
    "        if video['snippet']['title'][0] == 'E':                         # If it's viable\n",
    "            title = video['snippet']['title']                           # Defining the title\n",
    "            title = title.replace(\"Everything Wrong With \",\"\")          # Removing start of title\n",
    "            title = title.replace(\" Minutes Or Less\",\"\")                # Removing most of end of title\n",
    "            if title[-5:-3] == 'In':                                    # Removing \" In ##\"\n",
    "                title = title[:-6]\n",
    "            elif title[-4:-2] == 'In':\n",
    "                title = title[:-5]\n",
    "            sins_titles.append(title)\n",
    "        else:\n",
    "            sins_titles.append('Other Video')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two cells were used to make sure that all of the bojects in each list were alligned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sanity Check that the wins cypher list works\n",
    "count = 0\n",
    "for cypher in sorted(wins_cypher):\n",
    "    print('cypher: ',wins_cypher[count])\n",
    "    count += 1\n",
    "    print('count: ',count)\n",
    "    print('                      ', wins_titles[cypher])\n",
    "    print('                ', wins_tuples[cypher])\n",
    "    print(wins_video_list[cypher]['snippet']['title'])\n",
    "    print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check that the sins cypher list works\n",
    "count = 0\n",
    "for cypher in sorted(sins_cypher):\n",
    "    print('cypher: ',sins_cypher[count])\n",
    "    count += 1\n",
    "    print('count: ',count)\n",
    "    print('                     ', sins_titles[cypher])\n",
    "    print('              ',sins_tuples[cypher])\n",
    "    print(sins_video_list[cypher]['snippet']['title'])\n",
    "#    print(sins_video_list[cypher]['id']['videoId'])\n",
    "    print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two cells actually collected the comments from all vieos that we were looking for and printed out what video it was on at each step.  The outputs have been cleared for ease of reading but for example, the first few lines of the following cell were:\n",
    "\n",
    "The Avengers !!!\n",
    "\n",
    "CWVaNzGpQI8\n",
    "\n",
    "Page#: 1 comments: 20 Next: MVVvYTdDNE\n",
    "\n",
    "Page#: 2 comments: 40 Next: MklIZ3gwWH\n",
    "\n",
    "Page#: 3 comments: 60 Next: MUhuSUM2UX\n",
    "\n",
    "Page#: 4 comments: 80 Next: ME9EVEJfdF\n",
    "\n",
    "Page#: 5 comments: 100 Next: MFU5c1EtME\n",
    "\n",
    "Page#: 6 comments: 120 Next: MzRBRjdtbU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# WINS with Sins!\n",
    "list_of_items = []                                           # Instantiate list_of_items\n",
    "endpoint = 'https://www.googleapis.com/youtube/v3/commentThreads' \n",
    "for cypher in sorted(wins_cypher):\n",
    "    VideoId = wins_video_list[cypher]['id']['videoId']\n",
    "    first_params = {                                         # Instantiate first Parameters\n",
    "    'part':'id,snippet',\n",
    "    'videoId':str(VideoId),\n",
    "    'key':'AIzaSyCruFbHTolS_lK_AHrakQrjSVLzdEO5MaI'\n",
    "    }\n",
    "    params = first_params  \n",
    "    print(wins_titles[cypher],'!!!')\n",
    "    print(wins_video_list[cypher]['id']['videoId'])\n",
    "    for page in range(int(50000/20)):                        # For the number of pulls\n",
    "        try:\n",
    "            res=requests.get(url=endpoint,params=params)     # Makes requests, starting with 1st\n",
    "            for i in range(20):\n",
    "                list_of_items.append(res.json()['items'][i]) # Adds each item to list_of_items\n",
    "            next_page = res.json()['nextPageToken']          # Sets the next_page Token\n",
    "            params = {                                       # Re-defines parameters to pull next page\n",
    "                'part':'id,snippet',\n",
    "                'videoId':str(VideoId),\n",
    "                'key':'AIzaSyCruFbHTolS_lK_AHrakQrjSVLzdEO5MaI',\n",
    "                'pageToken' : next_page\n",
    "            }\n",
    "            print(\"Page#:\", page+1, \"comments:\", (page+1)*20) # Sanity Check\n",
    "\n",
    "        except:                                               # Try/Except to ensure we stop at end\n",
    "            print('Limit likely hit. Returning available posts.')\n",
    "            break\n",
    "list_of_dicts = []\n",
    "for item in range(len(list_of_items)):\n",
    "    quick_dict = {\n",
    "        \"comment_id\": list_of_items[item]['id'],\n",
    "        \"replies\": list_of_items[item]['snippet']['totalReplyCount'],\n",
    "        'author_id': list_of_items[item]['snippet']['topLevelComment']['snippet']['authorChannelId']['value'],\n",
    "        'author_name': list_of_items[item]['snippet']['topLevelComment']['snippet']['authorDisplayName'],\n",
    "        'likes': list_of_items[item]['snippet']['topLevelComment']['snippet']['likeCount'],\n",
    "        'published_time': list_of_items[item]['snippet']['topLevelComment']['snippet']['publishedAt'],\n",
    "        'text': list_of_items[item]['snippet']['topLevelComment']['snippet']['textOriginal'],\n",
    "        'video_id': list_of_items[item]['snippet']['topLevelComment']['snippet']['videoId']\n",
    "        }\n",
    "    list_of_dicts.append(quick_dict)\n",
    "wins_df = pd.DataFrame(list_of_dicts, columns = ['text', 'likes', 'replies', 'published_time', \n",
    "                                                 'comment_id', 'author_id', 'author_name', 'video_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SINS with Wins!\n",
    "list_of_items = []                                       # Instantiate list_of_items\n",
    "endpoint = 'https://www.googleapis.com/youtube/v3/commentThreads'\n",
    "params = first_params  \n",
    "for cypher in sorted(sins_cypher):\n",
    "    VideoId = sins_video_list[cypher]['id']['videoId']\n",
    "    first_params = {                                         # Instantiate first Parameters\n",
    "    'part':'id,snippet',\n",
    "    'videoId':str(VideoId),\n",
    "    'key':'AIzaSyCruFbHTolS_lK_AHrakQrjSVLzdEO5MaI'\n",
    "    }\n",
    "    params = first_params  \n",
    "    print(sins_titles[cypher],'!!!')\n",
    "    print(sins_video_list[cypher]['id']['videoId'])\n",
    "    for page in range(int(50000/20)):                 # For the number of pulls\n",
    "        try:\n",
    "            res=requests.get(url=endpoint,params=params)     # Makes requests, starting with 1st\n",
    "            for i in range(20):\n",
    "                list_of_items.append(res.json()['items'][i]) # Adds each item to list_of_items\n",
    "            next_page = res.json()['nextPageToken']          # Sets the next_page Token\n",
    "            params = {                                       # Re-defines parameters to pull next page\n",
    "                'part':'id,snippet',\n",
    "                'videoId':str(VideoId),\n",
    "                'key':'AIzaSyCruFbHTolS_lK_AHrakQrjSVLzdEO5MaI',\n",
    "                'pageToken' : next_page\n",
    "            }\n",
    "            print(\"Page#:\", page+1, \"comments:\", (page+1)*20) # Sanity Check\n",
    "\n",
    "        except:                                               # Try/Except to ensure we stop at end\n",
    "            print('Limit likely hit.  Returning available posts.')\n",
    "            break\n",
    "list_of_dicts = []\n",
    "for item in range(len(list_of_items)):\n",
    "    quick_dict = {\n",
    "        \"comment_id\": list_of_items[item]['id'],\n",
    "        \"replies\": list_of_items[item]['snippet']['totalReplyCount'],\n",
    "        'author_id': list_of_items[item]['snippet']['topLevelComment']['snippet']['authorChannelId']['value'],\n",
    "        'author_name': list_of_items[item]['snippet']['topLevelComment']['snippet']['authorDisplayName'],\n",
    "        'likes': list_of_items[item]['snippet']['topLevelComment']['snippet']['likeCount'],\n",
    "        'published_time': list_of_items[item]['snippet']['topLevelComment']['snippet']['publishedAt'],\n",
    "        'text': list_of_items[item]['snippet']['topLevelComment']['snippet']['textOriginal'],\n",
    "        'video_id': list_of_items[item]['snippet']['topLevelComment']['snippet']['videoId']\n",
    "        }\n",
    "    list_of_dicts.append(quick_dict)\n",
    "sins_df = pd.DataFrame(list_of_dicts, columns = ['text', 'likes', 'replies', 'published_time', \n",
    "                                                 'comment_id', 'author_id', 'author_name', 'video_id'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
