{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey, do these things for yourself.  Nobody else.  Get the most out of it for ourselves and ourselves only.\n",
    "\n",
    "All Labs have solutions now so we can go through "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importations and Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                    # Imports for Standard Junk\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np\n",
    "                                                    # Imports for Natural Language Processing\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from textblob import TextBlob\n",
    "#import json\n",
    "#from bs4 import BeautifulSoup\n",
    "                                                    # Imports for Plotting & Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "                                                    # Imports for Modeling\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "#from sklearn.naive_bayes import MultinomialNB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path Start: Json Scraping\n",
    "Sentiment:\n",
    "    Sentamentize videos\n",
    "    Sentamentize Channels\n",
    "    Correlations to sentament\n",
    "    Import more data\n",
    "    Compair sentament to other data of other channels\n",
    "    \n",
    "Topic Selection:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiations of the Preprocessor and Count Vectorizer\n",
    "from nltk.corpus import stopwords\n",
    "#    stop_words = stopwords.words(\"english\")\n",
    "#    return \" \".join([lemmer.lemmatize(word) for word in tokens if not word in stop_words])\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'[^a-zA-Z]',' ', text.lower())\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmer = WordNetLemmatizer()\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    return \" \".join([lemmer.lemmatize(word) for word in tokens if len(word) > 1 and not word in stop_words])\n",
    "cvec = CountVectorizer(analyzer = \"word\",\n",
    "                       min_df = 2,\n",
    "                       preprocessor = preprocess,\n",
    "                       stop_words = 'english') \n",
    "\n",
    "\n",
    "cvec_three = CountVectorizer(analyzer = \"word\",\n",
    "                       min_df = 2,\n",
    "                       preprocessor = preprocess,\n",
    "                       stop_words = 'english',\n",
    "                       ngram_range = (3,3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/terra/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Loading up all of the Channel Comments into dataframes\n",
    "Aliens_Guide         = pd.read_csv('./data/Aliens_Guide_9_25')\n",
    "Allison              = pd.read_csv('./data/Allisons_Channel_Comments_9_25')\n",
    "Hi_Im_Mary_Mary      = pd.read_csv('./data/Hi_Im_Mary_Mary_9_25')\n",
    "Maggie_Mae_Fish      = pd.read_csv('./data/Maggie_Mae_Fish_9_25')\n",
    "LaRon_Readus         = pd.read_csv('./data/LaRon_Readus_9_25')\n",
    "Strange_Aeons        = pd.read_csv('./data/Strange_Aeons_9_25')\n",
    "MovieBob             = pd.read_csv('./data/MovieBob_9_25')\n",
    "Sammus               = pd.read_csv('./data/Sammus_9_25')\n",
    "Shodan               = pd.read_csv('./data/Shodan_9_25')\n",
    "Saraj_Raval          = pd.read_csv('./data/Saraj_Raval_9_27_attempt3')\n",
    "Nyx_Fears            = pd.read_csv('./data/Nyx_Fears_9_25')\n",
    "Petscop              = pd.read_csv('./data/Petscop_9_25')\n",
    "NerdSync             = pd.read_csv('./data/NerdSync_9_25')\n",
    "Everyman_HYBRID      = pd.read_csv('./data/Everyman_HYBRID_9_25')\n",
    "Small_Beans          = pd.read_csv('./data/Small_Beans_9_25')\n",
    "Some_More_News       = pd.read_csv('./data/Some_More_News_9_25')\n",
    "The_Gaming_Brit_Show = pd.read_csv('./data/The_Gaming_Brit_Show_9_27')\n",
    "Super_Bunnyhop       = pd.read_csv('./data/Super_Bunnyhop_9_25')\n",
    "# Following added on Oct 21\n",
    "ContraPoints         = pd.read_csv('./data/ContraPoints_Channel_Comments_10_17')\n",
    "Lindsay_Ellis        = pd.read_csv('./data/Lindsay_Ellis_10_18')\n",
    "Queer_Kid_Stuff      = pd.read_csv('./data/Queer_Kid_Stuff_10_18')\n",
    "The_Golden_One       = pd.read_csv('./data/The_Golden_One_10_18')\n",
    "Ant_Dude             = pd.read_csv('./data/Ant_Dude_10_18')\n",
    "Black_Pilled         = pd.read_csv('./data/Black_Pilled_10_18')\n",
    "Magog_of_Morskar     = pd.read_csv('./data/Magog_of_Morskar_10_18')\n",
    "Polygon              = pd.read_csv('./data/Polygon_10_18')\n",
    "Radekken             = pd.read_csv('./data/Radekken_10_18')\n",
    "CinemaWins           = pd.read_csv('./data/CinemaWins_10_19')\n",
    "CinemaSins           = pd.read_csv('./data/CinemaSins_10_19')\n",
    "#                     = pd.read_csv('./data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549795\n"
     ]
    }
   ],
   "source": [
    "# First list is for all the channel names, second list is of all the dataframes of the channels in a list\n",
    "channel_names = ['Aliens_Guide', 'Allison', 'Ant_Dude', 'Black_Pilled', \n",
    "                 'CinemaSins', 'CinemaWins', 'ContraPoints', \n",
    "                 'Everyman_HYBRID', 'Hi_Im_Mary_Mary', 'LaRon_Readus', 'Lindsay_Ellis', \n",
    "                 'Maggie_Mae_Fish', 'Magog_of_Morskar', 'MovieBob', 'NerdSync', 'Nyx_Fears', \n",
    "                 'Petscop', 'Polygon', 'Queer_Kid_Stuff', 'Radekken', 'Sammus', 'Shodan', \n",
    "                 'Small_Beans', 'Some_More_News', 'Strange_Aeons', 'Super_Bunnyhop', \n",
    "                 'The_Gaming_Brit_Show', 'The_Golden_One']\n",
    "channels_obtained = [Aliens_Guide, Allison, Ant_Dude, Black_Pilled, \n",
    "                     CinemaSins, CinemaWins, ContraPoints, \n",
    "                     Everyman_HYBRID, Hi_Im_Mary_Mary, LaRon_Readus, Lindsay_Ellis, \n",
    "                     Maggie_Mae_Fish, Magog_of_Morskar, MovieBob, NerdSync, Nyx_Fears, \n",
    "                     Petscop,Polygon, Queer_Kid_Stuff, Radekken, Sammus, Shodan, \n",
    "                     Small_Beans, Some_More_News, Strange_Aeons, Super_Bunnyhop, \n",
    "                     The_Gaming_Brit_Show, The_Golden_One]\n",
    "\n",
    "# Current number of comments in all the channels that we currently have.\n",
    "count = 0\n",
    "for i in channels_obtained:\n",
    "    count += i.shape[0]\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentimentality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a sentimization function\n",
    "\n",
    "def Sentamentize(text):\n",
    "    return TextBlob(str(text)).sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 Aliens_Guide\n",
      "27 Allison\n",
      "26 Ant_Dude\n",
      "25 Black_Pilled\n",
      "24 CinemaSins\n",
      "23 CinemaWins\n",
      "22 ContraPoints\n",
      "21 Everyman_HYBRID\n",
      "20 Hi_Im_Mary_Mary\n",
      "19 LaRon_Readus\n",
      "18 Lindsay_Ellis\n",
      "17 Maggie_Mae_Fish\n",
      "16 Magog_of_Morskar\n",
      "15 MovieBob\n",
      "14 NerdSync\n",
      "13 Nyx_Fears\n",
      "12 Petscop\n",
      "11 Polygon\n",
      "10 Queer_Kid_Stuff\n",
      "9 Radekken\n",
      "8 Sammus\n",
      "7 Shodan\n",
      "6 Small_Beans\n",
      "5 Some_More_News\n",
      "4 Strange_Aeons\n",
      "3 Super_Bunnyhop\n",
      "2 The_Gaming_Brit_Show\n",
      "1 The_Golden_One\n"
     ]
    }
   ],
   "source": [
    "# Adding Sentimization to each row of all the existant dataframes\n",
    "\n",
    "count = 0\n",
    "for df in channels_obtained:\n",
    "    print(len(channels_obtained)-count, channel_names[count])\n",
    "    df['sentiment_textblob'] = df.apply(lambda x: pd.Series(Sentamentize(x),\n",
    "                                                            index=['text']),\n",
    "                                                            axis=1)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aliens_Guide 0.32087793017115235\n",
      "Allison 0.39108676046176044\n",
      "Ant_Dude 0.26618182570828824\n",
      "Black_Pilled 0.26857607052564225\n",
      "CinemaSins 0.26229829108655317\n",
      "CinemaWins 0.3104749125581729\n",
      "ContraPoints 0.29610938640502754\n",
      "Everyman_HYBRID 0.27206207241541824\n",
      "Hi_Im_Mary_Mary 0.29798546273410714\n",
      "LaRon_Readus 0.2949830476880584\n",
      "Lindsay_Ellis 0.29492783898301006\n",
      "Maggie_Mae_Fish 0.3457911574076005\n",
      "Magog_of_Morskar 0.2690056508754031\n",
      "MovieBob 0.28133110354098173\n",
      "NerdSync 0.29828911435786365\n",
      "Nyx_Fears 0.3286713994964437\n",
      "Petscop 0.25405123775820965\n",
      "Polygon 0.2045516500455746\n",
      "Queer_Kid_Stuff 0.2598593964334704\n",
      "Radekken 0.3039413402264928\n",
      "Sammus 0.3562067275436523\n",
      "Shodan 0.31600742304642127\n",
      "Small_Beans 0.31223313550288045\n",
      "Some_More_News 0.29285319756676076\n",
      "Strange_Aeons 0.26617750920026256\n",
      "Super_Bunnyhop 0.26944546654274304\n",
      "The_Gaming_Brit_Show 0.1509558670603739\n",
      "The_Golden_One 0.28038346397353753\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for df in channels_obtained:\n",
    "    print(channel_names[count], df['sentiment_textblob'].mean())\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cvec_MovieBob = CountVec(MovieBob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'sentament_textblob'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3062\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3063\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3064\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sentament_textblob'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-354eee9d24f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mMovieBob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentament_textblob'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2683\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2684\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2685\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2687\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2690\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2692\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2484\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2486\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2487\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   4113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4115\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4116\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4117\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3063\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3064\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3065\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3067\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sentament_textblob'"
     ]
    }
   ],
   "source": [
    "MovieBob['sentament_textblob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec_MovieBob['sentiment'] = MovieBob['sentiment_textblob']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cvec_MovieBob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c3d29c1d19f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlin_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'columns_etc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcvec_MovieBob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sentament'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcvec_MovieBob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentament'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cvec_MovieBob' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "features = ['columns_etc']\n",
    "X = cvec_MovieBob.drop('sentament', axis = 1)\n",
    "y = cvec_MovieBob['sentament']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "# Alt-code for SS\n",
    "lin_reg.fit(X_train, y_train) #these are the paramaeters for predictions\n",
    "lin_reg.score(X_test,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "X = MovieBob.drop('sentiment_textblob', axis = 1)\n",
    "y = MovieBob['sentiment_textblob']\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "# Alt-code for SS\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()         \n",
    "X_train_scaled = ss.fit_transform(X_train)  # Fit & Transform XTrain\n",
    "X_test_scaled = ss.transform(X_test)        # Transform XTest\n",
    "\n",
    "lin_reg.fit(X_train_scaled, y_train) #these are the paramaeters for predictions\n",
    "lin_reg.score(X_test_scaled,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.592659573403085e+27"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "features = ['columns_etc']\n",
    "X = cvec_MovieBob.drop('sentament', axis = 1)\n",
    "y = cvec_MovieBob['sentament']\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "# Alt-code for SS\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()         \n",
    "X_train_scaled = ss.fit_transform(X_train)  # Fit & Transform XTrain\n",
    "X_test_scaled = ss.transform(X_test)        # Transform XTest\n",
    "\n",
    "lin_reg.fit(X_train_scaled, y_train) #these are the paramaeters for predictions\n",
    "lin_reg.score(X_test_scaled,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>likes</th>\n",
       "      <th>replies</th>\n",
       "      <th>published_time</th>\n",
       "      <th>author_name</th>\n",
       "      <th>author_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>is_public</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>sentiment_textblob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I think snyder may hate  superman more than fr...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-09-25T14:35:16.000Z</td>\n",
       "      <td>magnasupreme</td>\n",
       "      <td>UClgdpWUFTdMCycAP9epZZdQ</td>\n",
       "      <td>F9juReoJxI0</td>\n",
       "      <td>True</td>\n",
       "      <td>Ugy94nV74jqnp5GWYU94AaABAg</td>\n",
       "      <td>0.016667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ah you're the escapist leech.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-09-25T12:40:24.000Z</td>\n",
       "      <td>Nathan</td>\n",
       "      <td>UCojZYAHsyKtmmPAp1AhSk3Q</td>\n",
       "      <td>RdkYCpQ2TNY</td>\n",
       "      <td>True</td>\n",
       "      <td>Ugx7pD5arLOlb-pc48Z4AaABAg</td>\n",
       "      <td>0.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why you gotta ruin YouTube as my culture Bob :(</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-09-25T08:04:14.000Z</td>\n",
       "      <td>Justin M</td>\n",
       "      <td>UCmIQGNT68vn9smJREtngedg</td>\n",
       "      <td>jrusTEvgxgI</td>\n",
       "      <td>True</td>\n",
       "      <td>UgxjSMC48JpT6AzqwMp4AaABAg</td>\n",
       "      <td>-0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Going rampant is fucking dumbass the new remak...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-09-24T18:27:51.000Z</td>\n",
       "      <td>Max Nyström</td>\n",
       "      <td>UCnitjL9AIhyNsg-wvIsiOXw</td>\n",
       "      <td>BPoILjs6BYI</td>\n",
       "      <td>True</td>\n",
       "      <td>UgyRbNXjv36cX2wbKNl4AaABAg</td>\n",
       "      <td>0.096591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It figures that one massive manchild would giv...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-09-24T18:23:49.000Z</td>\n",
       "      <td>ReverendSyn</td>\n",
       "      <td>UCdluLm6lTqnWGJXD3oR-2hw</td>\n",
       "      <td>oLvux3kpu1o</td>\n",
       "      <td>True</td>\n",
       "      <td>UgwM3C2AP-_fVUcBAGV4AaABAg</td>\n",
       "      <td>0.175000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  likes  replies  \\\n",
       "0  I think snyder may hate  superman more than fr...    0.0      0.0   \n",
       "1                      Ah you're the escapist leech.    0.0      0.0   \n",
       "2    Why you gotta ruin YouTube as my culture Bob :(    0.0      0.0   \n",
       "3  Going rampant is fucking dumbass the new remak...    0.0      0.0   \n",
       "4  It figures that one massive manchild would giv...    0.0      0.0   \n",
       "\n",
       "             published_time   author_name                 author_id  \\\n",
       "0  2018-09-25T14:35:16.000Z  magnasupreme  UClgdpWUFTdMCycAP9epZZdQ   \n",
       "1  2018-09-25T12:40:24.000Z        Nathan  UCojZYAHsyKtmmPAp1AhSk3Q   \n",
       "2  2018-09-25T08:04:14.000Z      Justin M  UCmIQGNT68vn9smJREtngedg   \n",
       "3  2018-09-24T18:27:51.000Z   Max Nyström  UCnitjL9AIhyNsg-wvIsiOXw   \n",
       "4  2018-09-24T18:23:49.000Z   ReverendSyn  UCdluLm6lTqnWGJXD3oR-2hw   \n",
       "\n",
       "      video_id is_public                  comment_id  sentiment_textblob  \n",
       "0  F9juReoJxI0      True  Ugy94nV74jqnp5GWYU94AaABAg            0.016667  \n",
       "1  RdkYCpQ2TNY      True  Ugx7pD5arLOlb-pc48Z4AaABAg            0.350000  \n",
       "2  jrusTEvgxgI      True  UgxjSMC48JpT6AzqwMp4AaABAg           -0.200000  \n",
       "3  BPoILjs6BYI      True  UgyRbNXjv36cX2wbKNl4AaABAg            0.096591  \n",
       "4  oLvux3kpu1o      True  UgwM3C2AP-_fVUcBAGV4AaABAg            0.175000  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MovieBob.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.592659573403085e+27"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "features = ['columns_etc']\n",
    "X = MovieBob.drop('sentament', axis = 1)\n",
    "y = MovieBob['sentament']\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "# Alt-code for SS\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()         \n",
    "X_train_scaled = ss.fit_transform(X_train)  # Fit & Transform XTrain\n",
    "X_test_scaled = ss.transform(X_test)        # Transform XTest\n",
    "\n",
    "lin_reg.fit(X_train_scaled, y_train) #these are the paramaeters for predictions\n",
    "lin_reg.score(X_test_scaled,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.18538887881918e+23"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()         \n",
    "X_train_scaled = ss.fit_transform(X_train)  # Fit & Transform XTrain\n",
    "X_test_scaled = ss.transform(X_test)        # Transform XTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>likes</th>\n",
       "      <th>replies</th>\n",
       "      <th>is_public</th>\n",
       "      <th>sentiment_textblob</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-WNfFu4AgxE</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.488889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3OwnPr_5JJA</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.271429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4j5d06YbU1o</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>0.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JqmWhozWOhw</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.254167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KeOtZghc194</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.616667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LbFhwWeysDI</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.475000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MgP_6GipDCU</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>None</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.355556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OIS4h-zqHFo</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.475000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PT-0fWqQe2w</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.487500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QOD57KWArzM</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.637500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flSTkbgISAw</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.244583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oeTIb49Pgm0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vcJgTzDBXko</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.437370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>veyfNvN-Dv8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.675000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             likes  replies  is_public  sentiment_textblob\n",
       "video_id                                                  \n",
       "-WNfFu4AgxE      0        0       True            0.488889\n",
       "3OwnPr_5JJA      0        0       True           -0.271429\n",
       "4j5d06YbU1o      0        2       True            0.350000\n",
       "JqmWhozWOhw      0        0       True            0.254167\n",
       "KeOtZghc194      0        0       True            0.616667\n",
       "LbFhwWeysDI      0        0       True            0.475000\n",
       "MgP_6GipDCU      0        0       True            0.350000\n",
       "None             0        0       True            0.355556\n",
       "OIS4h-zqHFo      0        0       True            0.475000\n",
       "PT-0fWqQe2w      0        0       True            0.487500\n",
       "QOD57KWArzM      0        0       True            0.637500\n",
       "flSTkbgISAw      0        0       True            0.244583\n",
       "oeTIb49Pgm0      0        0       True            0.350000\n",
       "vcJgTzDBXko      0        0       True            0.437370\n",
       "veyfNvN-Dv8      0        0       True            0.675000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Allison.groupby(['video_id']).mean()\n",
    "\n",
    "#df.groupby([video_id]).avg(df['sentiment_textblob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    incels_test = pd.read_csv('../../project-3/braincels_9_9_400')\n",
    "except:\n",
    "    incels_test = pd.read_csv('../../5th_week/project-3/braincels_9_9_400.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.075000\n",
       "2      0.175000\n",
       "19     0.350000\n",
       "21    -0.027500\n",
       "27     0.076020\n",
       "32    -0.170139\n",
       "35    -0.320000\n",
       "38     0.080000\n",
       "44    -0.100000\n",
       "46    -0.098864\n",
       "49     0.024359\n",
       "53     0.000000\n",
       "57    -0.045238\n",
       "62    -0.004688\n",
       "63    -0.280952\n",
       "69    -0.177273\n",
       "73    -0.012500\n",
       "74    -0.207692\n",
       "77     0.136364\n",
       "78     0.002500\n",
       "86     0.000000\n",
       "88     0.150000\n",
       "95     0.041667\n",
       "97     0.046333\n",
       "101    0.150000\n",
       "103    0.168750\n",
       "107    0.216667\n",
       "112    0.017560\n",
       "113   -0.035714\n",
       "117   -0.184444\n",
       "         ...   \n",
       "919    0.000000\n",
       "920    0.000000\n",
       "921    0.178571\n",
       "922    0.028889\n",
       "924    0.171296\n",
       "927   -0.375000\n",
       "928    0.500000\n",
       "929    0.014008\n",
       "930   -0.104242\n",
       "931    0.192857\n",
       "932    0.285185\n",
       "941    0.072222\n",
       "942    0.370312\n",
       "944    0.250000\n",
       "947    0.412500\n",
       "949    0.000000\n",
       "951    0.000000\n",
       "952   -0.284375\n",
       "956   -0.155556\n",
       "957    0.444444\n",
       "959   -0.245000\n",
       "961    0.000000\n",
       "966   -0.358333\n",
       "967    0.357143\n",
       "968   -0.260526\n",
       "970   -0.086458\n",
       "972   -0.156944\n",
       "973    0.250000\n",
       "979   -0.066667\n",
       "980    0.000000\n",
       "Length: 457, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incels_test.dropna().apply(lambda row: Sentamentize(incels_test['selftext'][row]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3062\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3063\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3064\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'text'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-93b99647be86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mincels_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2683\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2684\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2685\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2687\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2690\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2692\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2484\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2486\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2487\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   4113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4115\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4116\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4117\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3063\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3064\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3065\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3067\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'text'"
     ]
    }
   ],
   "source": [
    "incels_test['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sentamentize(incels_test['selftext'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>sentiment_textblob</th>\n",
       "      <th>sentiment_textblob_st</th>\n",
       "      <th>sentiment_textblob_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Goodbye, guys. I leave you all with this song.</td>\n",
       "      <td>I was going to do this on a throwaway and just...</td>\n",
       "      <td>Braincels</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Found a false flag IncelTear/AHS user. Watch o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Braincels</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>-0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>First day of kindergarten tomorrow? Any advice...</td>\n",
       "      <td>Sup guys I'm a 4 year old incel (got rejected ...</td>\n",
       "      <td>Braincels</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Joblless Female Geting Rejected</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Braincels</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“The past is the past guys”. Get over it</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Braincels</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>-0.250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0     Goodbye, guys. I leave you all with this song.   \n",
       "1  Found a false flag IncelTear/AHS user. Watch o...   \n",
       "2  First day of kindergarten tomorrow? Any advice...   \n",
       "3                    Joblless Female Geting Rejected   \n",
       "4           “The past is the past guys”. Get over it   \n",
       "\n",
       "                                            selftext  subreddit  \\\n",
       "0  I was going to do this on a throwaway and just...  Braincels   \n",
       "1                                                NaN  Braincels   \n",
       "2  Sup guys I'm a 4 year old incel (got rejected ...  Braincels   \n",
       "3                                                NaN  Braincels   \n",
       "4                                                NaN  Braincels   \n",
       "\n",
       "   sentiment_textblob  sentiment_textblob_st  sentiment_textblob_title  \n",
       "0               0.000                  0.000                     0.000  \n",
       "1              -0.400                 -0.400                    -0.400  \n",
       "2               0.175                  0.175                     0.175  \n",
       "3               0.250                  0.250                     0.250  \n",
       "4              -0.250                 -0.250                    -0.250  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incels_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CountVec(df):            # A quick and easy function for Count Vectorization into a data frame!\n",
    "    return pd.DataFrame(cvec.fit_transform(df['text']).todense(), columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for df in channels_obtained: # Dropping any rows of NaN.  There were less than 10 between all df's\n",
    "    print(channel_names[count],df.isna().sum().sum())\n",
    "    df.shape()\n",
    "    df['text'].dropna(inplace=True)\n",
    "    df.shape()\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/17 Done\n",
      "11/17 Done\n",
      "15/17 Done\n",
      "16/17 Done\n"
     ]
    }
   ],
   "source": [
    "                             # Count Vectorizing all current channels.\n",
    "cvec_Aliens_Guide = CountVec(Aliens_Guide)\n",
    "cvec_Allison = CountVec(Allison)\n",
    "cvec_Everyman_HYBRID = CountVec(Everyman_HYBRID)\n",
    "cvec_Hi_Im_Mary_Mary = CountVec(Hi_Im_Mary_Mary)\n",
    "cvec_LaRon_Readus = CountVec(LaRon_Readus)\n",
    "print('5/26 Done')\n",
    "cvec_Maggie_Mae_Fish = CountVec(Maggie_Mae_Fish)\n",
    "cvec_MovieBob = CountVec(MovieBob)\n",
    "cvec_NerdSync = CountVec(NerdSync)\n",
    "cvec_Nyx_Fears = CountVec(Nyx_Fears)\n",
    "cvec_Petscop = CountVec(Petscop)\n",
    "cvec_Sammus = CountVec(Sammus)\n",
    "print('11/26 Done')\n",
    "cvec_Shodan = CountVec(Shodan)\n",
    "cvec_Small_Beans = CountVec(Small_Beans)\n",
    "cvec_Some_More_News = CountVec(Some_More_News)\n",
    "cvec_Strange_Aeons = CountVec(Strange_Aeons)\n",
    "print('15/26 Done')\n",
    "cvec_Super_Bunnyhop = CountVec(Super_Bunnyhop)\n",
    "print('16/26 Done')\n",
    "cvec_The_Gaming_Brit_Show = CountVec(The_Gaming_Brit_Show)\n",
    "cvec_ContraPoints         = CountVec(ContraPoints)\n",
    "cvec_Lindsay_Ellis        = CountVec(Lindsay_Ellis)\n",
    "cvec_Queer_Kid_Stuff      = CountVec(Queer_Kid_Stuff)\n",
    "print('20/26 Done')\n",
    "cvec_The_Golden_One       = CountVec(The_Golden_One)\n",
    "cvec_Ant_Dude             = CountVec(Ant_Dude)\n",
    "cvec_Black_Pilled         = CountVec(Black_Pilled)\n",
    "cvec_Magog_of_Morskar     = CountVec(Magog_of_Morskar)\n",
    "print('24/26 Done')\n",
    "cvec_Polygon              = CountVec(Polygon)\n",
    "print('25/26 Done')\n",
    "cvec_Radekken             = CountVec(Radekken)\n",
    "\n",
    "\n",
    "channels_cveced = [cvec_Aliens_Guide,\n",
    " cvec_Allison, cvec_Everyman_HYBRID, cvec_Hi_Im_Mary_Mary, cvec_LaRon_Readus,\n",
    " cvec_Maggie_Mae_Fish, cvec_MovieBob, cvec_NerdSync, cvec_Nyx_Fears,\n",
    " cvec_Petscop, cvec_Sammus, cvec_Shodan, cvec_Small_Beans, cvec_Some_More_News,\n",
    " cvec_Strange_Aeons, cvec_Super_Bunnyhop, cvec_The_Gaming_Brit_Show]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-0f01842241af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchannels_cveced\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./cvec_data/cvec_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mchannel_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dsi/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[1;32m   1743\u001b[0m                                  \u001b[0mdoublequote\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoublequote\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1744\u001b[0m                                  escapechar=escapechar, decimal=decimal)\n\u001b[0;32m-> 1745\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dsi/lib/python3.6/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnicodeWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mwriter_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dsi/lib/python3.6/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dsi/lib/python3.6/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save_chunk\u001b[0;34m(self, start_i, end_i)\u001b[0m\n\u001b[1;32m    275\u001b[0m                                   \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                                   \u001b[0mdate_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                                   quoting=self.quoting)\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcol_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dsi/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mto_native_types\u001b[0;34m(self, slicer, na_rep, quoting, **kwargs)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_object\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mquoting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for df in channels_cveced:\n",
    "    df.to_csv('./cvec_data/cvec_' + channel_names[count], index=False)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec_ContraPoints         = CountVec(ContraPoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation\n",
    "\n",
    " Idea: Sentament Based on LDA\n",
    "         LDA Each row is a video\n",
    "         3 top topics\n",
    "         5 words per topic\n",
    "         cvec type columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation      # Import the LDA Model\n",
    "import warnings                                                  # Skip Depreccion Warnings\n",
    "def LDA_spelled_out(df, num_topics = 3, num_words = 5):\n",
    "    lda = LatentDirichletAllocation(n_components=num_topics)\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    cvec.fit(df['text'])\n",
    "    text = cvec.transform(df['text'])\n",
    "    lda.fit(text)\n",
    "    for ix, topic in enumerate(lda.components_):\n",
    "        print('Topic ', ix + 1)\n",
    "        top_words = [cvec.get_feature_names()[i] for i in lda.components_[ix].argsort()[:-num_words - 1:-1]]\n",
    "        print('\\n'.join(top_words))\n",
    "        print('')\n",
    "    return\n",
    "        \n",
    "#    return pd.DataFrame({'words':cvec.get_feature_names(),            # Actually printing the LDA stuff\n",
    "#             'LDA score': lda.components_[0]}).set_index('words').sort_values('LDA score',\n",
    "#                                                                              ascending =False).head(5)\n",
    "\n",
    "#cvec = CountVectorizer(analyzer = \"word\",\n",
    "#                             min_df = 2,\n",
    "#                             tokenizer = tokenizer.tokenize,\n",
    "#                             preprocessor = None,\n",
    "#                             stop_words = 'english') \n",
    "#lda = LatentDirichletAllocation(n_components=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aliens_Guide\n",
      "Topic  1\n",
      "people\n",
      "incredibles\n",
      "garyx\n",
      "time\n",
      "great\n",
      "\n",
      "Topic  2\n",
      "turkey\n",
      "monster\n",
      "mean\n",
      "spaghetti\n",
      "say\n",
      "\n",
      "Topic  3\n",
      "movie\n",
      "like\n",
      "alien\n",
      "guide\n",
      "video\n",
      "\n",
      "\n",
      "Allison\n",
      "Topic  1\n",
      "viola\n",
      "music\n",
      "great\n",
      "love\n",
      "play\n",
      "\n",
      "Topic  2\n",
      "music\n",
      "nice\n",
      "girl\n",
      "work\n",
      "amazing\n",
      "\n",
      "Topic  3\n",
      "violin\n",
      "nice\n",
      "guy\n",
      "music\n",
      "sheet\n",
      "\n",
      "\n",
      "Everyman_HYBRID\n",
      "Topic  1\n",
      "video\n",
      "shit\n",
      "fuck\n",
      "slenderman\n",
      "marble\n",
      "\n",
      "Topic  2\n",
      "evan\n",
      "like\n",
      "rake\n",
      "slenderman\n",
      "habit\n",
      "\n",
      "Topic  3\n",
      "slendy\n",
      "man\n",
      "slender\n",
      "guy\n",
      "need\n",
      "\n",
      "\n",
      "Hi_Im_Mary_Mary\n",
      "Topic  1\n",
      "mary\n",
      "hi\n",
      "hello\n",
      "love\n",
      "que\n",
      "\n",
      "Topic  2\n",
      "happy\n",
      "new\n",
      "video\n",
      "year\n",
      "real\n",
      "\n",
      "Topic  3\n",
      "garden\n",
      "mary\n",
      "think\n",
      "like\n",
      "know\n",
      "\n",
      "\n",
      "LaRon_Readus\n",
      "Topic  1\n",
      "people\n",
      "time\n",
      "like\n",
      "new\n",
      "watch\n",
      "\n",
      "Topic  2\n",
      "like\n",
      "movie\n",
      "video\n",
      "venom\n",
      "film\n",
      "\n",
      "Topic  3\n",
      "like\n",
      "movie\n",
      "character\n",
      "really\n",
      "think\n",
      "\n",
      "\n",
      "Maggie_Mae_Fish\n",
      "Topic  1\n",
      "video\n",
      "great\n",
      "maggie\n",
      "reaction\n",
      "love\n",
      "\n",
      "Topic  2\n",
      "movie\n",
      "like\n",
      "love\n",
      "think\n",
      "film\n",
      "\n",
      "Topic  3\n",
      "like\n",
      "look\n",
      "lol\n",
      "episode\n",
      "god\n",
      "\n",
      "\n",
      "MovieBob\n",
      "Topic  1\n",
      "movie\n",
      "like\n",
      "people\n",
      "film\n",
      "think\n",
      "\n",
      "Topic  2\n",
      "right\n",
      "gunn\n",
      "disney\n",
      "people\n",
      "james\n",
      "\n",
      "Topic  3\n",
      "bob\n",
      "like\n",
      "video\n",
      "good\n",
      "really\n",
      "\n",
      "\n",
      "NerdSync\n",
      "Topic  1\n",
      "like\n",
      "video\n",
      "spider\n",
      "man\n",
      "make\n",
      "\n",
      "Topic  2\n",
      "costume\n",
      "marvel\n",
      "need\n",
      "know\n",
      "mask\n",
      "\n",
      "Topic  3\n",
      "kid\n",
      "spiderman\n",
      "theme\n",
      "black\n",
      "best\n",
      "\n",
      "\n",
      "Nyx_Fears\n",
      "Topic  1\n",
      "movie\n",
      "like\n",
      "film\n",
      "really\n",
      "think\n",
      "\n",
      "Topic  2\n",
      "love\n",
      "nyx\n",
      "movie\n",
      "video\n",
      "review\n",
      "\n",
      "Topic  3\n",
      "video\n",
      "love\n",
      "happy\n",
      "like\n",
      "really\n",
      "\n",
      "\n",
      "Petscop\n",
      "Topic  1\n",
      "paul\n",
      "like\n",
      "care\n",
      "think\n",
      "marvin\n",
      "\n",
      "Topic  2\n",
      "game\n",
      "video\n",
      "theory\n",
      "paul\n",
      "nifty\n",
      "\n",
      "Topic  3\n",
      "petscop\n",
      "matpat\n",
      "fuck\n",
      "loss\n",
      "hey\n",
      "\n",
      "\n",
      "Sammus\n",
      "Topic  1\n",
      "love\n",
      "video\n",
      "song\n",
      "great\n",
      "music\n",
      "\n",
      "Topic  2\n",
      "like\n",
      "need\n",
      "think\n",
      "life\n",
      "sound\n",
      "\n",
      "Topic  3\n",
      "dope\n",
      "awesome\n",
      "thanks\n",
      "sammus\n",
      "rapper\n",
      "\n",
      "\n",
      "Shodan\n",
      "Topic  1\n",
      "vid\n",
      "burn\n",
      "guy\n",
      "na\n",
      "gon\n",
      "\n",
      "Topic  2\n",
      "nice\n",
      "book\n",
      "good\n",
      "got\n",
      "lol\n",
      "\n",
      "Topic  3\n",
      "yes\n",
      "love\n",
      "twilight\n",
      "vampire\n",
      "song\n",
      "\n",
      "\n",
      "Small_Beans\n",
      "Topic  1\n",
      "movie\n",
      "like\n",
      "love\n",
      "people\n",
      "stuff\n",
      "\n",
      "Topic  2\n",
      "game\n",
      "like\n",
      "think\n",
      "story\n",
      "thing\n",
      "\n",
      "Topic  3\n",
      "cracked\n",
      "guy\n",
      "love\n",
      "video\n",
      "content\n",
      "\n",
      "\n",
      "Some_More_News\n",
      "Topic  1\n",
      "news\n",
      "cody\n",
      "video\n",
      "love\n",
      "good\n",
      "\n",
      "Topic  2\n",
      "great\n",
      "cody\n",
      "like\n",
      "youtube\n",
      "straw\n",
      "\n",
      "Topic  3\n",
      "people\n",
      "trump\n",
      "right\n",
      "like\n",
      "left\n",
      "\n",
      "\n",
      "Strange_Aeons\n",
      "Topic  1\n",
      "people\n",
      "like\n",
      "love\n",
      "gay\n",
      "god\n",
      "\n",
      "Topic  2\n",
      "teacher\n",
      "like\n",
      "crush\n",
      "school\n",
      "year\n",
      "\n",
      "Topic  3\n",
      "girl\n",
      "time\n",
      "video\n",
      "long\n",
      "furby\n",
      "\n",
      "\n",
      "Super_Bunnyhop\n",
      "Topic  1\n",
      "game\n",
      "sonic\n",
      "year\n",
      "thought\n",
      "best\n",
      "\n",
      "Topic  2\n",
      "game\n",
      "like\n",
      "video\n",
      "vr\n",
      "good\n",
      "\n",
      "Topic  3\n",
      "game\n",
      "like\n",
      "world\n",
      "monster\n",
      "time\n",
      "\n",
      "\n",
      "The_Gaming_Brit_Show\n",
      "Topic  1\n",
      "announced\n",
      "review\n",
      "metroid\n",
      "wait\n",
      "thanks\n",
      "\n",
      "Topic  2\n",
      "dmc\n",
      "announced\n",
      "dante\n",
      "devil\n",
      "capcom\n",
      "\n",
      "Topic  3\n",
      "game\n",
      "like\n",
      "good\n",
      "really\n",
      "video\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for df in channels_obtained:\n",
    "    print(channel_names[count])\n",
    "    LDA_spelled_out(df, 3, 5)\n",
    "    print(\"\")\n",
    "    count += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
